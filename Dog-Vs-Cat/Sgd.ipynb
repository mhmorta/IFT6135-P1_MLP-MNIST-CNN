{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Data/trainset/trainset/Cat/'\n",
    "list_images = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mean and standard deviation of the ImageNet was: ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "# normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize((0.4895832, 0.4546405, 0.41594946), \n",
    "                                 (0.2520022, 0.24522494, 0.24728711))\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               normalize])\n",
    "traindata = torchvision.datasets.ImageFolder('./Data/trainset/trainset/', transform=transform)\n",
    "testset =torchvision.datasets.ImageFolder('./Data/testset/', transform=transform)\n",
    "\n",
    "\n",
    "size = [int(len(traindata)*0.9), len(traindata) - int(len(traindata)*0.9)]\n",
    "\n",
    "trainset, validset = torch.utils.data.random_split(dataset=traindata, lengths=size)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=128,\n",
    "                                          shuffle=True)\n",
    "\n",
    "validloader = torch.utils.data.DataLoader(validset,\n",
    "                                          batch_size=128,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                        batch_size=512,\n",
    "                                        shuffle=True)\n",
    "classes = ('Cat', 'Dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print (cuda_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17998\n",
      "Mean: [0.4895832  0.4546405  0.41594946]\n",
      "STD: [0.2520022  0.24522494 0.24728711]\n"
     ]
    }
   ],
   "source": [
    "## get the mean and variance of our dataset for normalization\n",
    "\n",
    "train = trainloader.__iter__().next()[0]\n",
    "print (len(trainloader.dataset))\n",
    "print('Mean: {}'.format(np.mean(train.numpy(), axis=(0, 2, 3))))\n",
    "print('STD: {}'.format(np.std(train.numpy(), axis=(0, 2, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = trainloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print('man injam')\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=16,kernel_size=(3,3) , padding=1),\n",
    "#             nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            # Layer 2\n",
    "            nn.Conv2d(in_channels=16, out_channels=32,kernel_size=(3,3) , padding=1),\n",
    "#             nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "            # Layer 3\n",
    "            nn.Conv2d(in_channels=32, out_channels=64,kernel_size=(3,3) , padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # Layer 4\n",
    "            nn.Conv2d(in_channels=64, out_channels=128,kernel_size=(3,3) , padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "            # Layer 5\n",
    "            nn.Conv2d(in_channels=128, out_channels=128,kernel_size=(3,3) , padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "            # Layer 6\n",
    "            nn.Conv2d(in_channels=128, out_channels=256,kernel_size=(3,3) , padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "             # Layer 7\n",
    "            nn.Conv2d(in_channels=256, out_channels=256,kernel_size=(3,3) , padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "            # Layer 8\n",
    "            nn.Conv2d(in_channels=256, out_channels=512,kernel_size=(3,3) , padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            # Layer 9\n",
    "            nn.Conv2d(in_channels=512, out_channels=512,kernel_size=(3,3) , padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "            \n",
    "        )\n",
    "        self.net = nn.Linear(512, 2)\n",
    "        self.soft = nn.Softmax()\n",
    "        self.init_weights()\n",
    "    def forward(self, x):\n",
    "        out = self.net(self.conv(x).squeeze())\n",
    "        return self.soft(out)\n",
    "    \n",
    "    def init_weights(self):\n",
    "#         if type(self.conv) == nn.Linear:\n",
    "#         print('man injam conv')\n",
    "#         torch.nn.init.xavier_uniform(self.conv.weight)\n",
    "        for m in self.conv:\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                nn.init.xavier_uniform(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "        if type(self.net) == nn.Linear:\n",
    "            nn.init.xavier_uniform(self.net.weight)\n",
    "            self.net.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:62: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:66: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "net = NNet()\n",
    "# net.apply(init_weights)\n",
    "learning_rate = 0.01\n",
    "if cuda_available:\n",
    "    net.cuda()\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=learning_rate, momentum=0.95)\n",
    "# optimizer = torch.optim.Adam(net.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 64, 64]             448\n",
      "              ReLU-2           [-1, 16, 64, 64]               0\n",
      "            Conv2d-3           [-1, 32, 64, 64]           4,640\n",
      "              ReLU-4           [-1, 32, 64, 64]               0\n",
      "         MaxPool2d-5           [-1, 32, 32, 32]               0\n",
      "            Conv2d-6           [-1, 64, 32, 32]          18,496\n",
      "              ReLU-7           [-1, 64, 32, 32]               0\n",
      "            Conv2d-8          [-1, 128, 32, 32]          73,856\n",
      "              ReLU-9          [-1, 128, 32, 32]               0\n",
      "        MaxPool2d-10          [-1, 128, 16, 16]               0\n",
      "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
      "             ReLU-12          [-1, 128, 16, 16]               0\n",
      "        MaxPool2d-13            [-1, 128, 8, 8]               0\n",
      "           Conv2d-14            [-1, 256, 8, 8]         295,168\n",
      "             ReLU-15            [-1, 256, 8, 8]               0\n",
      "        MaxPool2d-16            [-1, 256, 4, 4]               0\n",
      "           Conv2d-17            [-1, 256, 4, 4]         590,080\n",
      "             ReLU-18            [-1, 256, 4, 4]               0\n",
      "        MaxPool2d-19            [-1, 256, 2, 2]               0\n",
      "           Conv2d-20            [-1, 512, 2, 2]       1,180,160\n",
      "             ReLU-21            [-1, 512, 2, 2]               0\n",
      "           Conv2d-22            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-23            [-1, 512, 2, 2]               0\n",
      "        MaxPool2d-24            [-1, 512, 1, 1]               0\n",
      "           Linear-25                    [-1, 2]           1,026\n",
      "================================================================\n",
      "Total params: 4,671,266\n",
      "Trainable params: 4,671,266\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 7.48\n",
      "Params size (MB): 17.82\n",
      "Estimated Total Size (MB): 25.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net, (3,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_decay(epoch, loss, lr):\n",
    "    mean_loss = np.mean(loss[epoch-10:epoch-1])\n",
    "    if np.float(loss[epoch]) < np.float(mean_loss):\n",
    "        lr *= 0.1\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:54: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss : 0.694 \n",
      "Epoch: 0 Train Loss : 0.693 \n",
      "Epoch: 0 Train Loss : 0.693 \n",
      "Epoch: 0 Train Loss : 0.692 \n",
      "Epoch: 0 Train Loss : 0.690 \n",
      "Epoch: 0 Validation Acc : 55.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 1 Train Loss : 0.670 \n",
      "Epoch: 1 Train Loss : 0.663 \n",
      "Epoch: 1 Train Loss : 0.654 \n",
      "Epoch: 1 Train Loss : 0.654 \n",
      "Epoch: 1 Train Loss : 0.646 \n",
      "Epoch: 1 Validation Acc : 68.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 2 Train Loss : 0.566 \n",
      "Epoch: 2 Train Loss : 0.599 \n",
      "Epoch: 2 Train Loss : 0.605 \n",
      "Epoch: 2 Train Loss : 0.603 \n",
      "Epoch: 2 Train Loss : 0.603 \n",
      "Epoch: 2 Validation Acc : 70.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 3 Train Loss : 0.553 \n",
      "Epoch: 3 Train Loss : 0.576 \n",
      "Epoch: 3 Train Loss : 0.574 \n",
      "Epoch: 3 Train Loss : 0.574 \n",
      "Epoch: 3 Train Loss : 0.573 \n",
      "Epoch: 3 Validation Acc : 73.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 4 Train Loss : 0.575 \n",
      "Epoch: 4 Train Loss : 0.543 \n",
      "Epoch: 4 Train Loss : 0.547 \n",
      "Epoch: 4 Train Loss : 0.547 \n",
      "Epoch: 4 Train Loss : 0.549 \n",
      "Epoch: 4 Validation Acc : 73.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 5 Train Loss : 0.508 \n",
      "Epoch: 5 Train Loss : 0.544 \n",
      "Epoch: 5 Train Loss : 0.540 \n",
      "Epoch: 5 Train Loss : 0.537 \n",
      "Epoch: 5 Train Loss : 0.537 \n",
      "Epoch: 5 Validation Acc : 77.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 6 Train Loss : 0.493 \n",
      "Epoch: 6 Train Loss : 0.510 \n",
      "Epoch: 6 Train Loss : 0.512 \n",
      "Epoch: 6 Train Loss : 0.509 \n",
      "Epoch: 6 Train Loss : 0.506 \n",
      "Epoch: 6 Validation Acc : 77.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 7 Train Loss : 0.524 \n",
      "Epoch: 7 Train Loss : 0.490 \n",
      "Epoch: 7 Train Loss : 0.492 \n",
      "Epoch: 7 Train Loss : 0.488 \n",
      "Epoch: 7 Train Loss : 0.487 \n",
      "Epoch: 7 Validation Acc : 79.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 8 Train Loss : 0.480 \n",
      "Epoch: 8 Train Loss : 0.496 \n",
      "Epoch: 8 Train Loss : 0.482 \n",
      "Epoch: 8 Train Loss : 0.475 \n",
      "Epoch: 8 Train Loss : 0.474 \n",
      "Epoch: 8 Validation Acc : 79.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 9 Train Loss : 0.461 \n",
      "Epoch: 9 Train Loss : 0.470 \n",
      "Epoch: 9 Train Loss : 0.475 \n",
      "Epoch: 9 Train Loss : 0.470 \n",
      "Epoch: 9 Train Loss : 0.468 \n",
      "Epoch: 9 Validation Acc : 80.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 10 Train Loss : 0.430 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laya/.local/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/laya/.local/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train Loss : 0.449 \n",
      "Epoch: 10 Train Loss : 0.444 \n",
      "Epoch: 10 Train Loss : 0.449 \n",
      "Epoch: 10 Train Loss : 0.453 \n",
      "Epoch: 10 Validation Acc : 79.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 11 Train Loss : 0.446 \n",
      "Epoch: 11 Train Loss : 0.453 \n",
      "Epoch: 11 Train Loss : 0.448 \n",
      "Epoch: 11 Train Loss : 0.446 \n",
      "Epoch: 11 Train Loss : 0.443 \n",
      "Epoch: 11 Validation Acc : 79.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 12 Train Loss : 0.443 \n",
      "Epoch: 12 Train Loss : 0.434 \n",
      "Epoch: 12 Train Loss : 0.429 \n",
      "Epoch: 12 Train Loss : 0.424 \n",
      "Epoch: 12 Train Loss : 0.425 \n",
      "Epoch: 12 Validation Acc : 72.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 13 Train Loss : 0.576 \n",
      "Epoch: 13 Train Loss : 0.451 \n",
      "Epoch: 13 Train Loss : 0.438 \n",
      "Epoch: 13 Train Loss : 0.427 \n",
      "Epoch: 13 Train Loss : 0.423 \n",
      "Epoch: 13 Validation Acc : 80.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 14 Train Loss : 0.418 \n",
      "Epoch: 14 Train Loss : 0.395 \n",
      "Epoch: 14 Train Loss : 0.404 \n",
      "Epoch: 14 Train Loss : 0.407 \n",
      "Epoch: 14 Train Loss : 0.414 \n",
      "Epoch: 14 Validation Acc : 78.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 15 Train Loss : 0.412 \n",
      "Epoch: 15 Train Loss : 0.409 \n",
      "Epoch: 15 Train Loss : 0.404 \n",
      "Epoch: 15 Train Loss : 0.407 \n",
      "Epoch: 15 Train Loss : 0.405 \n",
      "Epoch: 15 Validation Acc : 79.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 16 Train Loss : 0.402 \n",
      "Epoch: 16 Train Loss : 0.418 \n",
      "Epoch: 16 Train Loss : 0.411 \n",
      "Epoch: 16 Train Loss : 0.404 \n",
      "Epoch: 16 Train Loss : 0.403 \n",
      "Epoch: 16 Validation Acc : 82.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 17 Train Loss : 0.381 \n",
      "Epoch: 17 Train Loss : 0.390 \n",
      "Epoch: 17 Train Loss : 0.392 \n",
      "Epoch: 17 Train Loss : 0.392 \n",
      "Epoch: 17 Train Loss : 0.396 \n",
      "Epoch: 17 Validation Acc : 80.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 18 Train Loss : 0.413 \n",
      "Epoch: 18 Train Loss : 0.396 \n",
      "Epoch: 18 Train Loss : 0.394 \n",
      "Epoch: 18 Train Loss : 0.395 \n",
      "Epoch: 18 Train Loss : 0.400 \n",
      "Epoch: 18 Validation Acc : 82.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 19 Train Loss : 0.380 \n",
      "Epoch: 19 Train Loss : 0.379 \n",
      "Epoch: 19 Train Loss : 0.379 \n",
      "Epoch: 19 Train Loss : 0.380 \n",
      "Epoch: 19 Train Loss : 0.385 \n",
      "Epoch: 19 Validation Acc : 78.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 20 Train Loss : 0.398 \n",
      "Epoch: 20 Train Loss : 0.397 \n",
      "Epoch: 20 Train Loss : 0.392 \n",
      "Epoch: 20 Train Loss : 0.388 \n",
      "Epoch: 20 Train Loss : 0.386 \n",
      "Epoch: 20 Validation Acc : 82.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 21 Train Loss : 0.341 \n",
      "Epoch: 21 Train Loss : 0.376 \n",
      "Epoch: 21 Train Loss : 0.377 \n",
      "Epoch: 21 Train Loss : 0.378 \n",
      "Epoch: 21 Train Loss : 0.378 \n",
      "Epoch: 21 Validation Acc : 81.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 22 Train Loss : 0.416 \n",
      "Epoch: 22 Train Loss : 0.378 \n",
      "Epoch: 22 Train Loss : 0.380 \n",
      "Epoch: 22 Train Loss : 0.379 \n",
      "Epoch: 22 Train Loss : 0.380 \n",
      "Epoch: 22 Validation Acc : 74.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 23 Train Loss : 0.448 \n",
      "Epoch: 23 Train Loss : 0.383 \n",
      "Epoch: 23 Train Loss : 0.380 \n",
      "Epoch: 23 Train Loss : 0.379 \n",
      "Epoch: 23 Train Loss : 0.378 \n",
      "Epoch: 23 Validation Acc : 82.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 24 Train Loss : 0.377 \n",
      "Epoch: 24 Train Loss : 0.376 \n",
      "Epoch: 24 Train Loss : 0.370 \n",
      "Epoch: 24 Train Loss : 0.371 \n",
      "Epoch: 24 Train Loss : 0.373 \n",
      "Epoch: 24 Validation Acc : 81.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 25 Train Loss : 0.345 \n",
      "Epoch: 25 Train Loss : 0.364 \n",
      "Epoch: 25 Train Loss : 0.368 \n",
      "Epoch: 25 Train Loss : 0.369 \n",
      "Epoch: 25 Train Loss : 0.370 \n",
      "Epoch: 25 Validation Acc : 81.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 26 Train Loss : 0.366 \n",
      "Epoch: 26 Train Loss : 0.362 \n",
      "Epoch: 26 Train Loss : 0.369 \n",
      "Epoch: 26 Train Loss : 0.374 \n",
      "Epoch: 26 Train Loss : 0.377 \n",
      "Epoch: 26 Validation Acc : 81.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 27 Train Loss : 0.375 \n",
      "Epoch: 27 Train Loss : 0.373 \n",
      "Epoch: 27 Train Loss : 0.373 \n",
      "Epoch: 27 Train Loss : 0.370 \n",
      "Epoch: 27 Train Loss : 0.372 \n",
      "Epoch: 27 Validation Acc : 81.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 28 Train Loss : 0.363 \n",
      "Epoch: 28 Train Loss : 0.366 \n",
      "Epoch: 28 Train Loss : 0.365 \n",
      "Epoch: 28 Train Loss : 0.366 \n",
      "Epoch: 28 Train Loss : 0.367 \n",
      "Epoch: 28 Validation Acc : 81.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n",
      "Epoch: 29 Train Loss : 0.395 \n",
      "Epoch: 29 Train Loss : 0.376 \n",
      "Epoch: 29 Train Loss : 0.377 \n",
      "Epoch: 29 Train Loss : 0.377 \n",
      "Epoch: 29 Train Loss : 0.375 \n",
      "Epoch: 29 Validation Acc : 82.000\n",
      "0.01\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "acc_val = []\n",
    "for epoch in range(30):\n",
    "    losses = []\n",
    "    # Train\n",
    "    net.train()\n",
    "    for batch_idx, (images, labels) in enumerate(trainloader):\n",
    "        if cuda_available:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.item())\n",
    "\n",
    "        \n",
    "        if batch_idx%30==0:\n",
    "            print('Epoch: %d Train Loss : %.3f ' % (epoch, np.mean(losses)))\n",
    "#             print('Epoch: %d Train Loss : %.3f ' % (epoch, loss.data.item()))\n",
    "    \n",
    "    # Evaluate\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (images, labels) in enumerate(validloader):\n",
    "        if cuda_available:\n",
    "            images, targets = images.cuda(), labels.cuda()\n",
    "\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        accuracy = 100.*(correct.numpy()/total)\n",
    "        acc_val.append(accuracy)\n",
    "\n",
    "    print('Epoch: %d Validation Acc : %.3f' % (epoch, 100.*correct/total))\n",
    "    print (learning_rate)\n",
    "    print('--------------------------------------------------------------')\n",
    "    net.train()\n",
    "    c += 1\n",
    "    if (c%10 == 0):\n",
    "        learning_rate = learning_rate_decay(epoch, acc_val, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './model/SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.953125\n",
      "52.604166666666664\n",
      "52.734375\n",
      "52.96875\n",
      "53.90625\n",
      "54.6875\n",
      "54.78515625\n",
      "54.921875\n",
      "54.947916666666664\n",
      "54.97159090909091\n",
      "55.12152777777778\n",
      "55.208333333333336\n",
      "55.24553571428571\n",
      "55.30133928571429\n",
      "55.40865384615385\n",
      "55.45\n",
      "67.96875\n",
      "68.54166666666667\n",
      "68.56971153846155\n",
      "68.61979166666666\n",
      "68.75\n",
      "68.75\n",
      "68.82102272727273\n",
      "68.90625\n",
      "68.9453125\n",
      "68.95\n",
      "69.0625\n",
      "69.140625\n",
      "69.35763888888889\n",
      "69.53125\n",
      "69.53125\n",
      "69.921875\n",
      "70.1\n",
      "70.26041666666667\n",
      "70.3125\n",
      "70.3125\n",
      "70.36830357142857\n",
      "70.44270833333334\n",
      "70.57291666666666\n",
      "70.703125\n",
      "70.75892857142857\n",
      "70.83333333333334\n",
      "70.8984375\n",
      "70.8984375\n",
      "70.9375\n",
      "71.00694444444444\n",
      "71.015625\n",
      "71.09375\n",
      "71.09375\n",
      "71.25\n",
      "71.484375\n",
      "71.54017857142857\n",
      "71.5625\n",
      "71.61458333333334\n",
      "71.61458333333334\n",
      "71.6796875\n",
      "71.6796875\n",
      "71.6796875\n",
      "71.70138888888889\n",
      "71.875\n",
      "71.875\n",
      "71.875\n",
      "71.875\n",
      "71.93080357142857\n",
      "72.08333333333333\n",
      "72.1\n",
      "72.109375\n",
      "72.265625\n",
      "72.265625\n",
      "72.30902777777779\n",
      "72.33072916666666\n",
      "72.35576923076923\n",
      "72.39583333333334\n",
      "72.52604166666666\n",
      "72.5360576923077\n",
      "72.54464285714286\n",
      "72.65625\n",
      "72.65625\n",
      "72.65625\n",
      "72.65625\n",
      "72.65625\n",
      "72.734375\n",
      "72.75390625\n",
      "72.7764423076923\n",
      "72.78645833333334\n",
      "72.93526785714286\n",
      "73.07291666666667\n",
      "73.1\n",
      "73.11197916666666\n",
      "73.17708333333334\n",
      "73.22443181818183\n",
      "73.38541666666667\n",
      "73.5\n",
      "73.69791666666666\n",
      "73.77232142857143\n",
      "73.828125\n",
      "73.828125\n",
      "73.90625\n",
      "73.95833333333334\n",
      "73.99553571428571\n",
      "74.21875\n",
      "74.27455357142857\n",
      "74.375\n",
      "74.4140625\n",
      "74.45\n",
      "74.47916666666666\n",
      "74.47916666666666\n",
      "74.6875\n",
      "74.82638888888889\n",
      "74.84375\n",
      "74.86979166666666\n",
      "74.92897727272727\n",
      "75.0\n",
      "75.0\n",
      "75.0\n",
      "75.0\n",
      "75.13020833333334\n",
      "75.3125\n",
      "75.52083333333334\n",
      "75.5859375\n",
      "75.78125\n",
      "75.78125\n",
      "75.78125\n",
      "76.30208333333334\n",
      "76.30208333333334\n",
      "76.4423076923077\n",
      "76.484375\n",
      "76.5625\n",
      "76.5625\n",
      "76.70454545454545\n",
      "76.7578125\n",
      "76.875\n",
      "76.953125\n",
      "76.953125\n",
      "76.953125\n",
      "77.03125\n",
      "77.06473214285714\n",
      "77.08333333333334\n",
      "77.3\n",
      "77.34375\n",
      "77.34375\n",
      "77.34375\n",
      "77.34375\n",
      "77.34375\n",
      "77.34375\n",
      "77.39583333333333\n",
      "77.45\n",
      "77.4639423076923\n",
      "77.734375\n",
      "77.734375\n",
      "77.734375\n",
      "77.79947916666666\n",
      "77.8125\n",
      "77.86458333333334\n",
      "77.86458333333334\n",
      "77.86458333333334\n",
      "77.90178571428571\n",
      "77.9296875\n",
      "78.125\n",
      "78.125\n",
      "78.125\n",
      "78.125\n",
      "78.125\n",
      "78.125\n",
      "78.125\n",
      "78.3203125\n",
      "78.33806818181817\n",
      "78.359375\n",
      "78.38541666666666\n",
      "78.38541666666666\n",
      "78.41796875\n",
      "78.41796875\n",
      "78.4375\n",
      "78.47222222222221\n",
      "78.57142857142857\n",
      "78.57142857142857\n",
      "78.64583333333334\n",
      "78.64583333333334\n",
      "78.64583333333334\n",
      "78.64583333333334\n",
      "78.68303571428571\n",
      "78.7\n",
      "78.72596153846155\n",
      "78.75\n",
      "78.75\n",
      "78.7860576923077\n",
      "78.80859375\n",
      "78.828125\n",
      "78.84114583333334\n",
      "78.90625\n",
      "78.90625\n",
      "78.90625\n",
      "78.90625\n",
      "78.90625\n",
      "78.90625\n",
      "78.95\n",
      "78.96205357142857\n",
      "78.97135416666666\n",
      "78.97727272727273\n",
      "78.97727272727273\n",
      "79.01785714285714\n",
      "79.01785714285714\n",
      "79.03645833333334\n",
      "79.0625\n",
      "79.07986111111111\n",
      "79.1015625\n",
      "79.1015625\n",
      "79.1015625\n",
      "79.1015625\n",
      "79.1015625\n",
      "79.140625\n",
      "79.14999999999999\n",
      "79.16666666666666\n",
      "79.16666666666666\n",
      "79.18526785714286\n",
      "79.1903409090909\n",
      "79.20673076923077\n",
      "79.21875\n",
      "79.21875\n",
      "79.21875\n",
      "79.21875\n",
      "79.23177083333334\n",
      "79.25347222222221\n",
      "79.26136363636364\n",
      "79.27083333333333\n",
      "79.296875\n",
      "79.296875\n",
      "79.296875\n",
      "79.3\n",
      "79.33238636363636\n",
      "79.4034090909091\n",
      "79.42708333333334\n",
      "79.44711538461539\n",
      "79.45\n",
      "79.46428571428571\n",
      "79.4921875\n",
      "79.4921875\n",
      "79.53125\n",
      "79.55729166666666\n",
      "79.5673076923077\n",
      "79.58333333333333\n",
      "79.609375\n",
      "79.609375\n",
      "79.6875\n",
      "79.6875\n",
      "79.6875\n",
      "79.6875\n",
      "79.6875\n",
      "79.6875\n",
      "79.6875\n",
      "79.6875\n",
      "79.6875\n",
      "79.75852272727273\n",
      "79.77430555555556\n",
      "79.80000000000001\n",
      "79.81770833333334\n",
      "79.81770833333334\n",
      "79.84375\n",
      "79.85491071428571\n",
      "79.86111111111111\n",
      "79.8828125\n",
      "79.8828125\n",
      "79.9\n",
      "79.91071428571429\n",
      "79.921875\n",
      "79.94791666666666\n",
      "79.94791666666666\n",
      "79.94791666666666\n",
      "79.98046875\n",
      "80.0\n",
      "80.0\n",
      "80.0\n",
      "80.01302083333334\n",
      "80.02232142857143\n",
      "80.04261363636364\n",
      "80.078125\n",
      "80.078125\n",
      "80.078125\n",
      "80.078125\n",
      "80.078125\n",
      "80.078125\n",
      "80.10000000000001\n",
      "80.10416666666667\n",
      "80.10416666666667\n",
      "80.10817307692307\n",
      "80.11363636363636\n",
      "80.15625\n",
      "80.15625\n",
      "80.15625\n",
      "80.16826923076923\n",
      "80.20833333333334\n",
      "80.20833333333334\n",
      "80.24553571428571\n",
      "80.24553571428571\n",
      "80.24553571428571\n",
      "80.33854166666666\n",
      "80.35714285714286\n",
      "80.37109375\n",
      "80.37109375\n",
      "80.40364583333334\n",
      "80.46875\n",
      "80.46875\n",
      "80.46875\n",
      "80.46875\n",
      "80.46875\n",
      "80.46875\n",
      "80.546875\n",
      "80.60000000000001\n",
      "80.625\n",
      "80.63616071428571\n",
      "80.6640625\n",
      "80.69196428571429\n",
      "80.72916666666666\n",
      "80.72916666666666\n",
      "80.78125\n",
      "80.80357142857143\n",
      "80.82386363636364\n",
      "80.82386363636364\n",
      "80.859375\n",
      "80.859375\n",
      "80.859375\n",
      "80.859375\n",
      "80.88942307692307\n",
      "80.90277777777779\n",
      "80.92447916666666\n",
      "80.94951923076923\n",
      "80.97098214285714\n",
      "80.98958333333334\n",
      "80.98958333333334\n",
      "80.98958333333334\n",
      "80.98958333333334\n",
      "81.00961538461539\n",
      "81.00961538461539\n",
      "81.03693181818183\n",
      "81.0546875\n",
      "81.0546875\n",
      "81.09375\n",
      "81.10000000000001\n",
      "81.11979166666666\n",
      "81.13839285714286\n",
      "81.14583333333333\n",
      "81.15234375\n",
      "81.17897727272727\n",
      "81.2\n",
      "81.25\n",
      "81.25\n",
      "81.25\n",
      "81.25\n",
      "81.25\n",
      "81.25\n",
      "81.25\n",
      "81.25\n",
      "81.25\n",
      "81.25\n",
      "81.25\n",
      "81.25\n",
      "81.25\n",
      "81.30208333333333\n",
      "81.31009615384616\n",
      "81.32102272727273\n",
      "81.35\n",
      "81.36160714285714\n",
      "81.38020833333334\n",
      "81.41741071428571\n",
      "81.4453125\n",
      "81.4453125\n",
      "81.47321428571429\n",
      "81.47321428571429\n",
      "81.51041666666666\n",
      "81.51041666666666\n",
      "81.59722222222221\n",
      "81.59722222222221\n",
      "81.6\n",
      "81.640625\n",
      "81.640625\n",
      "81.640625\n",
      "81.640625\n",
      "81.66666666666667\n",
      "81.68402777777779\n",
      "81.73076923076923\n",
      "81.77083333333334\n",
      "81.79086538461539\n",
      "81.8359375\n",
      "81.85\n",
      "81.85763888888889\n",
      "81.86383928571429\n",
      "81.875\n",
      "81.875\n",
      "81.875\n",
      "81.875\n",
      "81.875\n",
      "81.89999999999999\n",
      "81.90104166666666\n",
      "81.9110576923077\n",
      "81.9110576923077\n",
      "81.92708333333333\n",
      "81.92708333333333\n",
      "81.92708333333333\n",
      "81.96614583333334\n",
      "82.0\n",
      "82.0\n",
      "82.03125\n",
      "82.03125\n",
      "82.03125\n",
      "82.03125\n",
      "82.03125\n",
      "82.03125\n",
      "82.03125\n",
      "82.03125\n",
      "82.03125\n",
      "82.08333333333333\n",
      "82.08705357142857\n",
      "82.13541666666667\n",
      "82.14285714285714\n",
      "82.15\n",
      "82.2265625\n",
      "82.24431818181817\n",
      "82.25\n",
      "82.25446428571429\n",
      "82.25446428571429\n",
      "82.29166666666666\n",
      "82.29166666666666\n",
      "82.32421875\n",
      "82.35677083333334\n",
      "82.36607142857143\n",
      "82.39182692307693\n",
      "82.44791666666667\n",
      "82.45\n",
      "82.48697916666666\n",
      "82.5\n",
      "82.5284090909091\n",
      "82.55208333333334\n",
      "82.57211538461539\n",
      "82.63221153846155\n",
      "82.63888888888889\n",
      "82.64508928571429\n",
      "82.71484375\n",
      "82.734375\n",
      "82.74147727272727\n",
      "82.74739583333334\n",
      "82.8125\n",
      "82.8125\n",
      "82.8125\n",
      "82.890625\n",
      "82.89930555555556\n",
      "82.95454545454545\n",
      "82.96875\n",
      "83.07291666666666\n",
      "83.07291666666666\n",
      "83.14732142857143\n",
      "83.203125\n",
      "83.203125\n",
      "83.33333333333334\n",
      "83.37053571428571\n",
      "83.41346153846155\n",
      "83.4375\n",
      "83.52864583333334\n",
      "83.59375\n",
      "83.59375\n",
      "83.59375\n",
      "83.8778409090909\n",
      "83.90625\n",
      "83.984375\n",
      "83.984375\n",
      "83.984375\n",
      "84.08203125\n",
      "84.21875\n",
      "84.375\n",
      "84.375\n",
      "84.375\n",
      "84.375\n",
      "84.50520833333334\n",
      "84.63541666666666\n",
      "84.93303571428571\n",
      "85.15625\n",
      "85.15625\n",
      "85.15625\n",
      "85.3515625\n",
      "85.546875\n",
      "88.28125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88.28125"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.sort(acc_val)\n",
    "for i in range(len(a)):\n",
    "    print (a[i])\n",
    "np.max(acc_val)\n",
    "# print (losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(), \n",
    "#            os.path.join('saved_model', 'cnnT1_epoch_{}_iter_{}_loss_{}_acc_{}_{}.t7'.format(epoch + 1, i,\n",
    "#            last_loss, acc, datetime.datetime.now().strftime(\"%b_%d_%H:%M:%S\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_image = trainset[0\n",
    "# img = torch.from_numpy(np_image)\n",
    "# img = img.view(img.shape[1], img.shape[2], img.shape[0])\n",
    "# # img = img / 2+0.5\n",
    "# # npimg = img.numpy()\n",
    "# # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "# plt.imshow(img)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
